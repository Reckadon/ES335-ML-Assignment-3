{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9686832,"sourceType":"datasetVersion","datasetId":5921715},{"sourceId":9694677,"sourceType":"datasetVersion","datasetId":5926090},{"sourceId":143612,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":121689,"modelId":144823}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport pandas as pd\nimport matplotlib.pyplot as plt # for making figures\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nfrom pprint import pprint\nimport re\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T16:57:54.566833Z","iopub.execute_input":"2024-10-24T16:57:54.567218Z","iopub.status.idle":"2024-10-24T16:57:58.917100Z","shell.execute_reply.started":"2024-10-24T16:57:54.567179Z","shell.execute_reply":"2024-10-24T16:57:58.915778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sample data loading (replace this with any dataset URL)\n\nfilepath = '//kaggle/input/warpeace/warpeace.txt'\n# Load the text\nwith open(filepath, 'r', encoding='utf-8') as f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:57:58.919661Z","iopub.execute_input":"2024-10-24T16:57:58.920338Z","iopub.status.idle":"2024-10-24T16:57:59.125517Z","shell.execute_reply.started":"2024-10-24T16:57:58.920275Z","shell.execute_reply":"2024-10-24T16:57:59.124422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:57:59.126771Z","iopub.execute_input":"2024-10-24T16:57:59.127196Z","iopub.status.idle":"2024-10-24T16:57:59.184348Z","shell.execute_reply.started":"2024-10-24T16:57:59.127150Z","shell.execute_reply":"2024-10-24T16:57:59.183138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocess the text\ntext = text.lower()  # Convert to lowercase\ntext = re.sub(r'[^a-z\\s\\.]', '', text)  # Remove special characters except full stops\nwords = text.split()  # Split into words\nwords = [word for word in words if word]  # Remove empty strings\n\n# Remove words with less than 2 characters (for this example)\nwords = [word for word in words if len(word) > 1]\nprint(len(words))\nwords = pd.Series(words)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:57:59.189003Z","iopub.execute_input":"2024-10-24T16:57:59.190208Z","iopub.status.idle":"2024-10-24T16:57:59.646791Z","shell.execute_reply.started":"2024-10-24T16:57:59.190153Z","shell.execute_reply":"2024-10-24T16:57:59.645554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build vocabulary\nunique_words = sorted(set(words))\nstoi = {s: i + 1 for i, s in enumerate(unique_words)}  # Map words to indices\nstoi['.'] = 0  # End-of-sentence token\nitos = {i: s for s, i in stoi.items()}\n\n# Prepare input-output pairs\nblock_size = 5  # Number of words to use as context\nX, Y = [], []\n\nfor i in range(len(words) - block_size):\n    context = [0] * block_size  # Start with the end-of-sentence token\n    for j in range(block_size):\n        context[j] = stoi[words[i + j]]\n    X.append(context)\n    Y.append(stoi[words[i + block_size]])# The next word to predict\n\nprint(len(X))\nprint(len(Y))\nprint(itos[Y[-2]])","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:57:59.648416Z","iopub.execute_input":"2024-10-24T16:57:59.648884Z","iopub.status.idle":"2024-10-24T16:58:18.605802Z","shell.execute_reply.started":"2024-10-24T16:57:59.648832Z","shell.execute_reply":"2024-10-24T16:58:18.604662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = torch.tensor(X, dtype=torch.long)  # Input tensor\nY = torch.tensor(Y, dtype=torch.long)  # Output tensor","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:58:18.607157Z","iopub.execute_input":"2024-10-24T16:58:18.607518Z","iopub.status.idle":"2024-10-24T16:58:19.301385Z","shell.execute_reply.started":"2024-10-24T16:58:18.607481Z","shell.execute_reply":"2024-10-24T16:58:19.300193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:58:19.302940Z","iopub.execute_input":"2024-10-24T16:58:19.303425Z","iopub.status.idle":"2024-10-24T16:58:19.342389Z","shell.execute_reply.started":"2024-10-24T16:58:19.303367Z","shell.execute_reply":"2024-10-24T16:58:19.341314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define model parameters\nemb_dim = 64  # Larger embedding size for words\nhidden_size = 1024  # Larger hidden layer size\n\nclass NextWord(nn.Module):\n    def __init__(self, block_size, vocab_size, emb_dim, hidden_size):\n        super().__init__()\n        self.emb = nn.Embedding(vocab_size, emb_dim)\n        self.lin1 = nn.Linear(block_size * emb_dim, hidden_size)\n        self.lin2 = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x):\n        x = self.emb(x)\n        x = x.view(x.shape[0], -1)  # Flatten the embeddings\n        x = F.relu(self.lin1(x))  # Use ReLU activation\n        x = self.lin2(x)\n        return x\n\n# Initialize the model\nmodel = NextWord(block_size, len(stoi), emb_dim, hidden_size).to(device)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:58:19.343799Z","iopub.execute_input":"2024-10-24T16:58:19.344206Z","iopub.status.idle":"2024-10-24T16:58:19.816632Z","shell.execute_reply.started":"2024-10-24T16:58:19.344166Z","shell.execute_reply":"2024-10-24T16:58:19.815655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_word(model, itos, stoi, block_size, max_len=10):\n    context = [0] * block_size  # Start with end-of-sentence token\n    words_generated = []\n    \n    for _ in range(max_len):\n        x = torch.tensor(context).view(1, -1).to(device)\n        y_pred = model(x)\n        ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()\n        word = itos[ix]\n        if word == '.':\n            break\n        words_generated.append(word)\n        context = context[1:] + [ix]  # Update context\n\n    return ' '.join(words_generated)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:58:19.817745Z","iopub.execute_input":"2024-10-24T16:58:19.818120Z","iopub.status.idle":"2024-10-24T16:58:19.825988Z","shell.execute_reply.started":"2024-10-24T16:58:19.818081Z","shell.execute_reply":"2024-10-24T16:58:19.824824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss_fn = nn.CrossEntropyLoss()\n# opt = torch.optim.AdamW(model.parameters(), lr=0.01)\n\n# # Mini-batch training\n# batch_size = 13486\n# num_epochs = 1000  # You can train for more epochs\n# print_every = 100  # Print every 10 mini-batches\n\n# for epoch in range(num_epochs):\n#     # Shuffle data at the beginning of each epoch\n#     permutation = torch.randperm(len(X))\n#     X = X[permutation]\n#     Y = Y[permutation]\n\n#     for i in range(0, len(X) - (len(X)%batch_size), batch_size):\n#         x = X[i:i + batch_size].to(device)\n#         y = Y[i:i + batch_size].to(device)\n        \n#         # Forward pass\n#         y_pred = model(x)\n#         print('Done')\n        \n#         # Compute loss\n#         loss = loss_fn(y_pred, y)\n        \n#         # Backpropagation\n#         loss.backward()\n        \n#         # Gradient clipping\n#         torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n#         # Update weights\n#         opt.step()\n#         opt.zero_grad()\n\n#         # Print loss periodically\n#         if i % print_every == 0:\n#             print(f\"Epoch [{epoch}], Loss: {loss.item():.4f}\")\n    \n#     # Print loss after each epoch\n#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:58:19.830918Z","iopub.execute_input":"2024-10-24T16:58:19.831400Z","iopub.status.idle":"2024-10-24T16:58:19.840647Z","shell.execute_reply.started":"2024-10-24T16:58:19.831348Z","shell.execute_reply":"2024-10-24T16:58:19.839260Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# # torch.save(model, 'model.pth')\n# # torch.save(model.state_dict(), '/kaggle/working/next_word_model.pth')\n# # print(\"Model weights saved to /kaggle/working/next_word_model.pth\")\n# # Function to preprocess the input sentence\n# def preprocess_input(sentence, stoi, block_size):\n#     sentence = sentence.lower()  # Convert to lowercase\n#     sentence = re.sub(r'[^a-z\\s\\.]', '', sentence)  # Remove special characters except full stops\n#     words = sentence.split()  # Split into words\n#     words = [word for word in words if word]  # Remove empty strings\n\n#     # Create context from the last block_size words\n#     context = [0] * block_size  # Start with the end-of-sentence token\n#     for i in range(block_size):\n#         if i < len(words):\n#             word = words[i]\n#             context[i] = stoi.get(word, 0)  # Get the index, use 0 if word not in vocab\n#         else:\n#             context[i] = 0  # Fill with end-of-sentence token if fewer words\n\n#     return context\n\n# # Input sentence\n# input_sentence = \"can we really say that\"\n\n# # Preprocess the input sentence\n# context = preprocess_input(input_sentence, stoi, block_size)\n\n# # Convert context to tensor and send to device\n# context_tensor = torch.tensor(context, dtype=torch.long).view(1, -1).to(device)\n\n# # Generate the next word\n# with torch.no_grad():  # Disable gradient calculation for inference\n#     y_pred = model(context_tensor)  # Forward pass\n#     ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()  # Sample from the distribution\n#     next_word = itos[ix]  # Convert index to word\n\n# print(f\"The predicted next word for the input '{input_sentence}' is: '{next_word}'\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:58:19.842075Z","iopub.execute_input":"2024-10-24T16:58:19.842531Z","iopub.status.idle":"2024-10-24T16:58:19.851610Z","shell.execute_reply.started":"2024-10-24T16:58:19.842478Z","shell.execute_reply":"2024-10-24T16:58:19.850419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    mcopy = torch.load('/kaggle/input/next-word-predictor/pytorch/default/1/model.pth')\nexcept:\n    mcopy = torch.load('/kaggle/input/next-word-predictor/pytorch/default/1/model.pth',map_location=device)\nmcopy.eval()\n# Function to preprocess the input sentence\ndef preprocess_input(sentence, stoi, block_size):\n    sentence = sentence.lower()  # Convert to lowercase\n    sentence = re.sub(r'[^a-z\\s\\.]', '', sentence)  # Remove special characters except full stops\n    words = sentence.split()  # Split into words\n    words = [word for word in words if word]  # Remove empty strings\n\n    # Create context from the last block_size words\n    context = [0] * block_size  # Start with the end-of-sentence token\n    for i in range(block_size):\n        if i < len(words):\n            word = words[i]\n            context[i] = stoi.get(word, 0)  # Get the index, use 0 if word not in vocab\n        else:\n            context[i] = 0  # Fill with end-of-sentence token if fewer words\n\n    return context\n\n# Input sentence\ninput_sentence = \"men fought very bravely but\"\n\n# Preprocess the input sentence\ncontext = preprocess_input(input_sentence, stoi, block_size)\n\n# Convert context to tensor and send to device\ncontext_tensor = torch.tensor(context, dtype=torch.long).view(1, -1).to(device)\n\n# Generate the next word\nwith torch.no_grad():  # Disable gradient calculation for inference\n    y_pred = mcopy(context_tensor)  # Forward pass\n    ix = torch.distributions.categorical.Categorical(logits=y_pred).sample().item()  # Sample from the distribution\n    next_word = itos[ix]  # Convert index to word\n\nprint(f\"The predicted next word for the input '{input_sentence}' is: '{next_word}'\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:58:19.853153Z","iopub.execute_input":"2024-10-24T16:58:19.853603Z","iopub.status.idle":"2024-10-24T16:58:21.757010Z","shell.execute_reply.started":"2024-10-24T16:58:19.853563Z","shell.execute_reply":"2024-10-24T16:58:21.755863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n# Extract word embeddings from the trained model\ndef extract_embeddings(model, vocab_size):\n    print('Reached Function')\n    with torch.no_grad():\n        embeddings = model.emb.weight.cpu().numpy()  # Get the embeddings from the model\n    return embeddings\n\n# Get the word embeddings from the trained model\nembeddings = extract_embeddings(model, len(stoi))\n\n# Apply t-SNE to reduce the dimensionality of embeddings for visualization\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nprint('Calling fit-transform')\nembeddings_2d = tsne.fit_transform(embeddings)\nprint('fit-transform Done')\n\n# Plot the t-SNE results\nplt.figure(figsize=(12, 10))\nprint('initialing plotting')\nfor i, word in enumerate(itos.values()):\n    print(f'Plotting{i}th point')\n    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], marker='o', color='red')\n    plt.text(embeddings_2d[i, 0] + 0.01, embeddings_2d[i, 1] + 0.01, word, fontsize=9)\nprint('Done plotting')\nplt.title(\"t-SNE visualization of word embeddings\")\nplt.xlabel(\"t-SNE component 1\")\nplt.ylabel(\"t-SNE component 2\")\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:02:15.974306Z","iopub.execute_input":"2024-10-24T17:02:15.975165Z","iopub.status.idle":"2024-10-24T17:11:18.275961Z","shell.execute_reply.started":"2024-10-24T17:02:15.975117Z","shell.execute_reply":"2024-10-24T17:11:18.273468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the t-SNE results\nplt.figure(figsize=(12, 10))\nprint('initialing plotting')\nfor i, word in enumerate(itos.values()):\n    if i == 1000:\n        break\n    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], marker='o', color='red')\n    plt.text(embeddings_2d[i, 0] + 0.01, embeddings_2d[i, 1] + 0.01, word, fontsize=9)\nprint('Done plotting')\nplt.title(\"t-SNE visualization of word embeddings\")\nplt.xlabel(\"t-SNE component 1\")\nplt.ylabel(\"t-SNE component 2\")\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T17:13:59.154954Z","iopub.execute_input":"2024-10-24T17:13:59.155331Z","iopub.status.idle":"2024-10-24T17:14:19.240191Z","shell.execute_reply.started":"2024-10-24T17:13:59.155297Z","shell.execute_reply":"2024-10-24T17:14:19.238928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install streamlit","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:59:59.306981Z","iopub.status.idle":"2024-10-24T16:59:59.307474Z","shell.execute_reply.started":"2024-10-24T16:59:59.307229Z","shell.execute_reply":"2024-10-24T16:59:59.307252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import streamlit as st\nst.title(\"Next Word Prediction App\")\ninput_text = st.text_input(\"Enter initial text for prediction:\", \"I want to have\")\ncontext_length = st.selectbox(\"Context Length\", [5,10])\nembedding_dim = st.selectbox(\"Embedding Dimension\", [8,64,256])\nhidden_size = st.selectbox(\"Hidden Size\", [1024,2048])\nactivation_func_name = st.selectbox(\"Activation Function\", [\"ReLU\", \"Tanh\"])\nmax_len = st.selectbox(\"Number of words to predict\", [1,5])\nrandom_seed = st.number_input(\"Random Seed\", min_value=0, max_value=100, value=42)\nif st.button(\"Generate Text\"):\n    generated_text = generate_text(model, itos, stoi, context_length, input_text, max_len=num_words, device='cpu')\n    st.write(\"Generated text:\", input_text + \" \" + generated_text)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:59:59.308773Z","iopub.status.idle":"2024-10-24T16:59:59.309227Z","shell.execute_reply.started":"2024-10-24T16:59:59.309010Z","shell.execute_reply":"2024-10-24T16:59:59.309036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!streamlit run /opt/conda/lib/python3.10/site-packages/ipykernel_launcher.py","metadata":{"execution":{"iopub.status.busy":"2024-10-24T16:59:59.311190Z","iopub.status.idle":"2024-10-24T16:59:59.311621Z","shell.execute_reply.started":"2024-10-24T16:59:59.311403Z","shell.execute_reply":"2024-10-24T16:59:59.311424Z"},"trusted":true},"execution_count":null,"outputs":[]}]}